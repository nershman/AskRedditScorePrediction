{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Toulouse School of Economics**\n",
    "#### **M2 Statistics & Econometrics**\n",
    "---\n",
    "## **Web Mining**\n",
    "### Final Project\n",
    "---\n",
    "#### Anh-Dung LE\n",
    "#### Sherman ALINE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and setting for multiprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, unidecode, re, os, json, time, scipy.sparse\n",
    "import matplotlib as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms.traversal.depth_first_search import dfs_tree\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer as stemmer\n",
    "from multiprocessing import dummy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "core = os.cpu_count() - 1\n",
    "client = Client(n_workers = core, threads_per_worker = 1, memory_limit = '25GB')\n",
    "P = dummy.Pool(processes = core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:\\\\M2 EconStat\\\\Web Mining\\\\Project\\\\comments_students.csv'\n",
    "df = pd.read_csv(path, header = 0, engine = 'c',\n",
    "                 usecols = ['created_utc', 'ups', 'link_id', 'id', 'author', 'body', 'parent_id'])\n",
    "\n",
    "# We remove prefix so that columns \"parent_id\" and \"link_id\" have the same format as column \"id\"\n",
    "df['parent_id'] = df['parent_id'].str.split('_', expand = True)[1]\n",
    "df['link_id'] = df['link_id'].str.split('_', expand = True)[1]\n",
    "\n",
    "# We save this dataset for later use\n",
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\prefix_removed.csv'\n",
    "df.to_csv(path, index = False, encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the root for every spanning tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\prefix_removed.csv'\n",
    "df = pd.read_csv(path, header = 0, engine = 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we work with subgraphs characterized by `link_id`, then some features (for example, depth of a node w.r.t the *root*) can not be computed because a subgraph possibly contains sub-subgraphs that are disconnected from each others. That's why we refine the definition of a *root*. The idea is that a comment is a root if it does not reply any comment in the dataset. Hence its `in_degree` is $0$. Of course, a comment with `link_id` = `parent_id` is certainly a root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df, source = 'parent_id', target = 'id', create_using = nx.DiGraph())\n",
    "roots = [n for n, d in G.in_degree() if d == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `para_func` below takes a root as input and returns a dataframe. Column `id` contains all the nodes of the tree characterized by the root. We extract this tree by `dfs_tree`. All elements in column `root` are the same, i.e the root itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_func(r):\n",
    "    node = list(dfs_tree(G, r).nodes)\n",
    "    tree = G.subgraph(node)\n",
    "    node.remove(r)\n",
    "    root =  pd.DataFrame({'id': node, 'root': [r] * len(node)})\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation is not computationally intensive, so I use `multiprocessing.dummy.Pool` for simple implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node_list = P.map(para_func, roots)\n",
    "root_node = pd.concat(root_node_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add this root-node information to original dataframe. Notice that the original dataframe does not contain comments that are roots. That's why we specify `how = 'left'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(root_node, how = 'left', on = 'id')\n",
    "\n",
    "# We save this dataset for later use\n",
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\root_included.csv'\n",
    "df.to_csv(path, index = False, encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for the choice of tree-related features\n",
    "\n",
    "In this section,\n",
    "\n",
    "- I use the words \"node\" and \"comment\" interchangably.\n",
    "\n",
    "- If not stated otherwise, I meant by \"the tree\" the one characterized by the **root** of the comment.\n",
    "\n",
    "Then\n",
    "\n",
    "- `depth`: the shortest path from its root to the comment.\n",
    "\n",
    "If the `depth` is too large, the comments are less likely to be seen by other users, and thus less likely to be upvoted or downvoted.\n",
    "\n",
    "- `num_siblings`: the number of nodes within the tree that share the same parent.\n",
    "\n",
    "- `num_comments_author`: the number of comments made by the author within the tree. If the author made more than $1$ comments within the tree, then these comments have the same value of `num_comments_author`.\n",
    "\n",
    "- `num_previous_comments`: the number of nodes whose posting time is stricly sooner than that of the comment.\n",
    "\n",
    "- `num_later_comments`: the number of nodes whose posting time is stricly later than that of the comment.\n",
    "\n",
    "I add `num_siblings`, `num_comments_author`, `num_previous_comments`, and `num_later_comments` because I have seen them in the paper [Conversation Modeling on Reddit Using a Graph-Structured LSTM](https://www.aclweb.org/anthology/Q18-1009.pdf).\n",
    "\n",
    "- `time_since_root`: the interval of posting time between the comment and its **root**.\n",
    "\n",
    "- `time_since_parent`: the interval of posting time between the comment and its direct parent.\n",
    "\n",
    "If `time_since_root` or `time_since_parent` is small, then the comment are more likely to be seen by later users, and thus more likely to be upvoted or downvoted.\n",
    "\n",
    "- `num_comments_subtree`: the number of nodes in the tree.\n",
    "\n",
    "- `height_subtree`: the longest (directed) path in the tree.\n",
    "\n",
    "- `num_children`: the number of **direct** replies to the comment.\n",
    "\n",
    "`num_comments_subtree`, `height_subtree`, and `num_children` are size proxies of the tree rooted from the comment. It seems that interesting comments attracts more replies and thus its induced tree has bigger size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract tree-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\root_included.csv'\n",
    "df = dd.read_csv(path, header = 0)\n",
    "\n",
    "# We create column \"timestamp\" to compute time interval later\n",
    "df['timestamp'] = dd.to_datetime(df['created_utc'], utc = True, unit = 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not find any package containing functions to compute all features, except for *depth of the comment*, *height of the subtree*, and *size of that subtree*. Luckily, our trees are of a special kind, [arborescence](https://www.wikiwand.com/en/Tree_(graph_theory)#/Rooted_tree). This allows us to utilize package `dask` and highly optimized functions from package `pandas` to speed up computation. First, we compute features related to the tree characterized by column `root`. Comments with the same `root` are in the same tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extract(sub_df):\n",
    "    r = sub_df['root'].iloc[0]\n",
    "    tree = nx.from_pandas_edgelist(sub_df,\n",
    "                                   source = 'parent_id',\n",
    "                                   target = 'id',\n",
    "                                   create_using = nx.DiGraph())\n",
    "\n",
    "    count_utc = sub_df.groupby('created_utc').size()\n",
    "    cum_previous_counts = count_utc.sort_index(ascending = True).shift(fill_value = 0).cumsum()\n",
    "    cum_later_counts = count_utc.sort_index(ascending = False).shift(fill_value = 0).cumsum()\n",
    "    time_parent = sub_df.parent_id.map(dict(zip(sub_df.id, sub_df.timestamp)))\n",
    "\n",
    "    depth = [nx.shortest_path_length(tree, source = r, target = n) for n in sub_df.id]\n",
    "    num_siblings = sub_df.groupby('parent_id').parent_id.transform('count')\n",
    "    num_children = sub_df.groupby('parent_id').size().reindex(sub_df.id, fill_value = 0)\n",
    "    num_comments_author = sub_df.groupby('author').author.transform('count')\n",
    "    num_previous_comments = sub_df.created_utc.map(cum_previous_counts)\n",
    "    num_later_comments = sub_df.created_utc.map(cum_later_counts)\n",
    "    time_since_root = (sub_df.timestamp - sub_df.timestamp.min()) / pd.Timedelta(hours = 1)\n",
    "    time_since_parent = ((sub_df.timestamp - time_parent) / pd.Timedelta(hours = 1)).fillna(0)\n",
    "\n",
    "    data = list(zip(sub_df.id, depth, num_siblings, num_children, num_comments_author,\n",
    "                    num_previous_comments, num_later_comments, time_since_root, time_since_parent))\n",
    "\n",
    "    columns = ['id', 'depth', 'num_siblings', 'num_children', 'num_comments_author',\n",
    "              'num_previous_comments', 'num_later_comments', 'time_since_root', 'time_since_parent']\n",
    "\n",
    "    return pd.DataFrame(data, columns = columns)\n",
    "\n",
    "meta = {'id': object, 'depth': np.int64, 'num_siblings': np.int64, 'num_children': np.int64,\n",
    "       'num_comments_author': np.int64, 'num_previous_comments': np.int64, 'num_later_comments': np.int64,\n",
    "       'time_since_root': np.float64, 'time_since_parent': np.float64}\n",
    "\n",
    "result = df.groupby('root').apply(features_extract, meta = meta)\n",
    "result = result.compute(scheduler = 'processes')\n",
    "result.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>depth</th>\n",
       "      <th>num_siblings</th>\n",
       "      <th>num_children</th>\n",
       "      <th>num_comments_author</th>\n",
       "      <th>num_previous_comments</th>\n",
       "      <th>num_later_comments</th>\n",
       "      <th>time_since_root</th>\n",
       "      <th>time_since_parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cr1qq1w</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cqvdyoy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cqxivp8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cqxiruo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cqws27u</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234965</th>\n",
       "      <td>cquiajn</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234966</th>\n",
       "      <td>cqumskx</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.025556</td>\n",
       "      <td>2.025556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234967</th>\n",
       "      <td>cquxm86</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11.754444</td>\n",
       "      <td>9.728889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234968</th>\n",
       "      <td>cqugv6d</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234969</th>\n",
       "      <td>crjrvf6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4234970 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  depth  num_siblings  num_children  num_comments_author  \\\n",
       "0        cr1qq1w      1             1             0                    1   \n",
       "1        cqvdyoy      1             1             0                    1   \n",
       "2        cqxivp8      1             1             0                    1   \n",
       "3        cqxiruo      1             1             0                    1   \n",
       "4        cqws27u      1             1             0                    1   \n",
       "...          ...    ...           ...           ...                  ...   \n",
       "4234965  cquiajn      1             1             1                    1   \n",
       "4234966  cqumskx      2             1             1                    1   \n",
       "4234967  cquxm86      3             1             0                    1   \n",
       "4234968  cqugv6d      1             1             0                    1   \n",
       "4234969  crjrvf6      1             1             0                    1   \n",
       "\n",
       "         num_previous_comments  num_later_comments  time_since_root  \\\n",
       "0                            0                   0         0.000000   \n",
       "1                            0                   0         0.000000   \n",
       "2                            0                   0         0.000000   \n",
       "3                            0                   0         0.000000   \n",
       "4                            0                   0         0.000000   \n",
       "...                        ...                 ...              ...   \n",
       "4234965                      0                   2         0.000000   \n",
       "4234966                      1                   1         2.025556   \n",
       "4234967                      2                   0        11.754444   \n",
       "4234968                      0                   0         0.000000   \n",
       "4234969                      0                   0         0.000000   \n",
       "\n",
       "         time_since_parent  \n",
       "0                 0.000000  \n",
       "1                 0.000000  \n",
       "2                 0.000000  \n",
       "3                 0.000000  \n",
       "4                 0.000000  \n",
       "...                    ...  \n",
       "4234965           0.000000  \n",
       "4234966           2.025556  \n",
       "4234967           9.728889  \n",
       "4234968           0.000000  \n",
       "4234969           0.000000  \n",
       "\n",
       "[4234970 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark 1:*\n",
    "\n",
    "- I use `.compute()` or equivalently `.compute(scheduler = 'threads')`. The executing time is reduced to $30$ minutes. The utilization of CPU increases from $25\\%$ to $50\\%$. Previously, I used `multiprocessing.dummy.Pool()` and my laptop could not finish the computation in 3 hours. I had no choice but to interupt Python kernel.\n",
    "\n",
    "- Then I come across this [answer](https://stackoverflow.com/a/31364127/7357673) and change to `.compute(scheduler = 'processes')`. The utilization of CPU is almost $100\\%$. The executing time is reduced to just $12$ minutes. This is awesome.\n",
    "\n",
    "*Remark 2:*\n",
    "\n",
    "- At first I thought `dask` only works with function that returns a dictionary. This means I have to do $2$ more steps, i.e. converting resulted dictionaries to pandas dataframe and then concatenating them together. Even I used `multiprocessing.dummy.Pool()` for the conversion, it still took up to $2.5$ minutes.\n",
    "\n",
    "- Then I come across this [documentation](https://docs.dask.org/en/latest/dataframe-design.html#metadata). It tells that `dask` works perfectly with function that returns a pandas dataframe (It should ^_^). We only need to specify the correct data types with parameter `meta`. One advantage of this approach is that `dask` automatically concatenate resulted dataframes.\n",
    "\n",
    "*Remark 3:*\n",
    "\n",
    "- Because we have no information about the root, except for its `id`, we can not compute exactly `time_since_root`. Similarly, if the parent of a comment is itself a root, then `time_since_parent` can not be computed for this comment. Our solution is to take the earliest available posting time within a tree as a proxy for posting time of the root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of comments and height of the subtree rooted from the comment itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first create a graph from the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df,\n",
    "                            source = 'parent_id',\n",
    "                            target = 'id',\n",
    "                            create_using = nx.DiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `multiprocessing.dummy.Pool` and `dask.Series.apply` take almost the same amount of time (about $3$ minutes and $45$ seconds) to finish. This makes sense because `features_extract` is applied on **every** comment in the dataset. Hence `dask.Series.apply` does not have any advantage by cleverly partitioning the dataframe. Last but not least, `dask.Series.apply` has a disadvantage, i.e. it can not be called on an **existing** Dask dataframe. To use it, we need to import our dataset again with `pd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments_subtree</th>\n",
       "      <th>height_subtree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cqug90j</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cqug90k</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cqug90z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cqug91c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cqug91e</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234965</th>\n",
       "      <td>crrbelu</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234966</th>\n",
       "      <td>crrbelv</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234967</th>\n",
       "      <td>crrbemp</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234968</th>\n",
       "      <td>crrbenh</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234969</th>\n",
       "      <td>crrbeop</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4234970 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  num_comments_subtree  height_subtree\n",
       "0        cqug90j                     1               0\n",
       "1        cqug90k                     1               0\n",
       "2        cqug90z                     1               0\n",
       "3        cqug91c                     1               0\n",
       "4        cqug91e                     3               2\n",
       "...          ...                   ...             ...\n",
       "4234965  crrbelu                     1               0\n",
       "4234966  crrbelv                     1               0\n",
       "4234967  crrbemp                     1               0\n",
       "4234968  crrbenh                     1               0\n",
       "4234969  crrbeop                     1               0\n",
       "\n",
       "[4234970 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def features_extract(r):\n",
    "    subtree = dfs_tree(G, r)\n",
    "    num_comment = subtree.number_of_nodes()\n",
    "    height_subtree = nx.dag_longest_path_length(subtree)\n",
    "    return [r, num_comment, height_subtree]\n",
    "\n",
    "result2 = P.map(features_extract, df.id)\n",
    "result2 = pd.DataFrame(result2, columns = ['id', 'num_comments_subtree', 'height_subtree'])\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add these features to our original daraframe. By construction, `df`, `compute_result`, and `sub_tree_infor` have exactly the same elements in column `id`. That's why dont need to specify parameter `how`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>link_id</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>root</th>\n",
       "      <th>depth</th>\n",
       "      <th>num_siblings</th>\n",
       "      <th>num_children</th>\n",
       "      <th>num_comments_author</th>\n",
       "      <th>num_previous_comments</th>\n",
       "      <th>num_later_comments</th>\n",
       "      <th>time_since_root</th>\n",
       "      <th>time_since_parent</th>\n",
       "      <th>num_comments_subtree</th>\n",
       "      <th>height_subtree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34f9rh</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>jesse9o3</td>\n",
       "      <td>No one has a European accent either  because i...</td>\n",
       "      <td>cqug2sr</td>\n",
       "      <td>cqug2sr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34fvry</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>beltfedshooter</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>34fvry</td>\n",
       "      <td>34fvry</td>\n",
       "      <td>1</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34ffo5</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>InterimFatGuy</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>cqu80zb</td>\n",
       "      <td>cqu80zb</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34aqsn</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>JuanTutrego</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing g...</td>\n",
       "      <td>cqtdj4m</td>\n",
       "      <td>cqtdj4m</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>101.0</td>\n",
       "      <td>34f9rh</td>\n",
       "      <td>cqug91e</td>\n",
       "      <td>dcblackbelt</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, ...</td>\n",
       "      <td>cquc4rc</td>\n",
       "      <td>cquc4rc</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234965</th>\n",
       "      <td>1433116795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37y5rx</td>\n",
       "      <td>crrbelu</td>\n",
       "      <td>monster860</td>\n",
       "      <td>Does the sun set in the west/rise in the east?...</td>\n",
       "      <td>37y5rx</td>\n",
       "      <td>37y5rx</td>\n",
       "      <td>1</td>\n",
       "      <td>1496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3265</td>\n",
       "      <td>0</td>\n",
       "      <td>12.452222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234966</th>\n",
       "      <td>1433116795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37y5rx</td>\n",
       "      <td>crrbelv</td>\n",
       "      <td>jsimo36</td>\n",
       "      <td>Coffee.</td>\n",
       "      <td>37y5rx</td>\n",
       "      <td>37y5rx</td>\n",
       "      <td>1</td>\n",
       "      <td>1496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3265</td>\n",
       "      <td>0</td>\n",
       "      <td>12.452222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234967</th>\n",
       "      <td>1433116796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>380jx2</td>\n",
       "      <td>crrbemp</td>\n",
       "      <td>torianicole731</td>\n",
       "      <td>people who cannot make up their mind, my bulls...</td>\n",
       "      <td>380jx2</td>\n",
       "      <td>380jx2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234968</th>\n",
       "      <td>1433116797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37yawp</td>\n",
       "      <td>crrbenh</td>\n",
       "      <td>bellypouch</td>\n",
       "      <td>Give them to Irish people in exchange for doin...</td>\n",
       "      <td>37yawp</td>\n",
       "      <td>37yawp</td>\n",
       "      <td>1</td>\n",
       "      <td>1779</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3519</td>\n",
       "      <td>0</td>\n",
       "      <td>11.398056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234969</th>\n",
       "      <td>1433116799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37zhvy</td>\n",
       "      <td>crrbeop</td>\n",
       "      <td>NotByAnyMeans</td>\n",
       "      <td>classy way to put it</td>\n",
       "      <td>crragip</td>\n",
       "      <td>37zhvy</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>5.230000</td>\n",
       "      <td>0.501944</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4234970 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_utc    ups link_id       id          author  \\\n",
       "0         1430438400    3.0  34f9rh  cqug90j        jesse9o3   \n",
       "1         1430438400    3.0  34fvry  cqug90k  beltfedshooter   \n",
       "2         1430438400    5.0  34ffo5  cqug90z   InterimFatGuy   \n",
       "3         1430438401    1.0  34aqsn  cqug91c     JuanTutrego   \n",
       "4         1430438401  101.0  34f9rh  cqug91e     dcblackbelt   \n",
       "...              ...    ...     ...      ...             ...   \n",
       "4234965   1433116795    NaN  37y5rx  crrbelu      monster860   \n",
       "4234966   1433116795    NaN  37y5rx  crrbelv         jsimo36   \n",
       "4234967   1433116796    NaN  380jx2  crrbemp  torianicole731   \n",
       "4234968   1433116797    NaN  37yawp  crrbenh      bellypouch   \n",
       "4234969   1433116799    NaN  37zhvy  crrbeop   NotByAnyMeans   \n",
       "\n",
       "                                                      body parent_id     root  \\\n",
       "0        No one has a European accent either  because i...   cqug2sr  cqug2sr   \n",
       "1         That the kid ..reminds me of Kevin.   so sad :-(    34fvry   34fvry   \n",
       "2                                                     NSFL   cqu80zb  cqu80zb   \n",
       "3        I'm a guy and I had no idea this was a thing g...   cqtdj4m  cqtdj4m   \n",
       "4        Mid twenties male rocking skinny jeans/pants, ...   cquc4rc  cquc4rc   \n",
       "...                                                    ...       ...      ...   \n",
       "4234965  Does the sun set in the west/rise in the east?...    37y5rx   37y5rx   \n",
       "4234966                                           Coffee.     37y5rx   37y5rx   \n",
       "4234967  people who cannot make up their mind, my bulls...    380jx2   380jx2   \n",
       "4234968  Give them to Irish people in exchange for doin...    37yawp   37yawp   \n",
       "4234969                               classy way to put it   crragip   37zhvy   \n",
       "\n",
       "         depth  num_siblings  num_children  num_comments_author  \\\n",
       "0            1             1             0                    1   \n",
       "1            1          1443             0                    1   \n",
       "2            1             5             0                    1   \n",
       "3            1             4             0                    1   \n",
       "4            1             2             1                    1   \n",
       "...        ...           ...           ...                  ...   \n",
       "4234965      1          1496             0                    1   \n",
       "4234966      1          1496             0                    1   \n",
       "4234967      1             9             0                    1   \n",
       "4234968      1          1779             0                    1   \n",
       "4234969      7             1             0                    1   \n",
       "\n",
       "         num_previous_comments  num_later_comments  time_since_root  \\\n",
       "0                            0                   0         0.000000   \n",
       "1                            0                3512         0.000000   \n",
       "2                            0                  11         0.000000   \n",
       "3                            0                   4         0.000000   \n",
       "4                            0                   3         0.000000   \n",
       "...                        ...                 ...              ...   \n",
       "4234965                   3265                   0        12.452222   \n",
       "4234966                   3265                   0        12.452222   \n",
       "4234967                      8                   0         0.522500   \n",
       "4234968                   3519                   0        11.398056   \n",
       "4234969                    258                   0         5.230000   \n",
       "\n",
       "         time_since_parent  num_comments_subtree  height_subtree  \n",
       "0                 0.000000                     1               0  \n",
       "1                 0.000000                     1               0  \n",
       "2                 0.000000                     1               0  \n",
       "3                 0.000000                     1               0  \n",
       "4                 0.000000                     3               2  \n",
       "...                    ...                   ...             ...  \n",
       "4234965           0.000000                     1               0  \n",
       "4234966           0.000000                     1               0  \n",
       "4234967           0.000000                     1               0  \n",
       "4234968           0.000000                     1               0  \n",
       "4234969           0.501944                     1               0  \n",
       "\n",
       "[4234970 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\root_included.csv'\n",
    "df = pd.read_csv(path, header = 0, engine = 'c')\n",
    "df = df.merge(result, on = 'id')\n",
    "df = df.merge(result2, on = 'id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save it for later use\n",
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\features_included.csv'\n",
    "df.to_csv(path, index = False, encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features\n",
    "\n",
    "As a user of reddit, I will apply some domain knowledge in the process of selecting text features. \n",
    "\n",
    "- Emojis - emoticons may be used to detect sentiment. Additionally, they may signal a overly-casual post which may not be recieved well.\n",
    "\n",
    "- Sentiment in post - funny comments, as well as serious and informative posts will get upvotes. In long comment threads, a hostile tone may get upvotes (for one party, while the other party with hostile tone will get many downvotes).\n",
    "\n",
    "- Length - long comments may be an indication of high-information or high-effort post.\n",
    "\n",
    "- #Punctuation - this is another indicator that a post is likely to be well-written, correlated with useful information and understandable writing etc. All things which will get upvotes on AskReddit. Extremely large amounts of punctuation may also indicate visual text jokes being made using markdown.\n",
    "\n",
    "- #Paragraphs - indicator of high amiount of content/contribution or high effort. We detect paragraphs by the number of double spaces. Doublespaces are removed on reddit. Through manual investigation, I have determined that double spaces in our corpora are new lines.\n",
    "\n",
    "- Capital letters.\n",
    "\n",
    "- Number of links in a post - citations generally get upvotes, an indicator of sharing information.\n",
    "\n",
    "- Refer to other users `/u/*`.\n",
    "\n",
    "- Refer to subreditt `/r/`.\n",
    "\n",
    "- Quotes - detect, count number of them, remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\features_included.csv'\n",
    "df = pd.read_csv(path, header = 0, engine = 'c')\n",
    "ind = (df['body'] != 'deleted') & (df['body'] != '[deleted]') & df['body'].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ind` is the indices of  \"normal\" rows that will be fed into the model later. Next we define some strings used in formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "newline = '  '\n",
    "lessthan = '&lt;'\n",
    "greaterthan = '&gt;'\n",
    "emojis = '[\\\\u203C-\\\\u3299\\\\U0001F000-\\\\U0001F644]'\n",
    "hyperlink = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All manually added text features have the prefix `txt_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count then remove the number of markdown links\n",
    "df['txt_md_link_count'] = df.body.str.count('\\[[^\\]]+\\]\\([^\\)]+\\)')\n",
    "df['body'] = df.body.str.replace('\\[(?P<txt>[^\\]]+)\\]\\([^\\)]+\\)', '\\g<txt>')\n",
    "\n",
    "# Count then remove the number of links with no markdown\n",
    "df['txt_nomk_link_count'] = df.body.str.count(hyperlink)\n",
    "df['body'] = df.body.str.replace(hyperlink, '') \n",
    "\n",
    "# Reddit links: count then remove the /r/ part but keep the name of sub\n",
    "df['txt_reddit_links'] = df.body.str.count('/r/')\n",
    "df['body'] = df.body.str.replace('/r/', '')\n",
    "\n",
    "# User links \n",
    "df['txt_reddit_links'] = df.body.str.count('/u/')\n",
    "df['body'] = df.body.str.replace('/u/', '')\n",
    "\n",
    "# Count quotes and remove them\n",
    "df['txt_num_quotes'] = df.body.str.count(greaterthan+'(.+)'+newline)\n",
    "df['body'] = df.body.str.replace(greaterthan+'(.+)'+newline, 'QQUUOOTTEE ') # We use custom token and replcae w e.g. $$QQUUOOTTEE$$\n",
    "\n",
    "# Count emojis in each comment and remove them\n",
    "df['txt_num_emoji'] = df.body.str.count(emojis)\n",
    "df['body'] = df.body.str.replace('('+emojis+')', '')\n",
    "\n",
    "# Count capitals \n",
    "df['txt_num_caps'] = df.body.str.count('[A-Z]') # Signal good grammar\n",
    "\n",
    "# Comment length\n",
    "df['txt_length'] = df.body.str.len() # Relate to count capitals\n",
    "\n",
    "# Exclamation marks\n",
    "df['txt_excmarks'] = df.body.str.count('\\!') # People usually overuse\n",
    "\n",
    "# Question marks\n",
    "df['txt_qmarks'] = df.body.str.count('\\?') # People usually overuse\n",
    "\n",
    "# Other common punctuation.\n",
    "df['txt_punct'] = df.body.str.count('[\\-\\\"\\'\\,]') # The post is well-written\n",
    "\n",
    "# Number related marks\n",
    "df['txt_mathmarks'] = df.body.str.count('[\\$\\%\\+\\=]') # For comments more technical\n",
    "\n",
    "# Numbers\n",
    "df['txt_numerals'] = df.body.str.count('\\d') # For comments more technical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_stemming(x):\n",
    "    x = x.split()\n",
    "    x = [stemmer().stem(w) for w in x if w not in my_stopwords]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "tmp = df['body'][ind].copy()\n",
    "\n",
    "# Lowercase\n",
    "tmp = tmp.str.lower()\n",
    "\n",
    "# Delete punctuation\n",
    "tmp = tmp.str.replace('[\\\\n\\\\r\\/\\-]', ' ')\n",
    "tmp = tmp.str.replace(r\"((?!{}).)\".format('(\\\\b[-/]\\\\b|[a-zA-Z0-9])'), ' ', regex = True)\n",
    "\n",
    "#Tokenize and stemming\n",
    "tmp = P.map(tokenize_stemming, tmp)\n",
    "\n",
    "body = df.body.copy()\n",
    "body[ind] = tmp\n",
    "df['body'] = body\n",
    "\n",
    "# We save it for later use\n",
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\stemming.csv'\n",
    "df.to_csv(path, index = False, encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Our idea is as follows. In order to disregard words which are commonly used, we first generate TF-IDF and then drop features with low variance. We create a large set of features and then use feature selection to shrink it to a manageable number.\n",
    "\n",
    "Before stemming: no terms with 0.2 min df exist. We choose max_df = 0.01 because it is the highest df which removes grammar words such as i've etc, leaving more unique words.\n",
    "\n",
    "After stemming: extremely aggressive stemming algorithnm (many words no longer human readable)\n",
    "but still: no terms with 0.2 min df exist.\n",
    "\n",
    "Through manual investigation of words and document frequency,\n",
    "we decide on a min_df of __ and a max_df of ___\n",
    "\n",
    "We see that for above the max_df > 0.018, there were common words with little signal such as\n",
    "kind yeah play best talk thought etc... (check at max_df=0.8, display 800:850 of 1000 features\n",
    "\n",
    "For below the min_df < 1400, there were many proper nouns and slang such as bieber, meow, morrowind, obama, yooooo, and hubby, runescape\n",
    "\n",
    "With these constaints, we only get 3001 features, so we loosen them a bit and then remove irrelevant ones later with feature selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\stemming.csv'\n",
    "df = pd.read_csv(path, header = 0, engine = 'c')\n",
    "\n",
    "n_feature = 5000\n",
    "tfi_df_vec = TfidfVectorizer(use_idf = True,\n",
    "                             min_df = 0.00011,\n",
    "                             max_df = 0.018, \n",
    "                             stop_words = 'english',\n",
    "                             max_features = n_feature)\n",
    "\n",
    "data = tfi_df_vec.fit_transform(df['body'].fillna(''))\n",
    "thresh = 0.001\n",
    "sel = VarianceThreshold(threshold = thresh)\n",
    "features = sel.fit_transform(data)\n",
    "features = pd.DataFrame.sparse.from_spmatrix(features)\n",
    "df2 = pd.concat([df, features], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test  = df.ups.isna()\n",
    "ind_train = ~ ind_test\n",
    "X = df2.iloc[:, 8:].fillna(0)\n",
    "y = df.ups\n",
    "X_train, y_train = X.loc[ind_train, :], y[ind_train]\n",
    "X_test, y_test = X.loc[ind_test, :], y[ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_full = df.ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Because training set contains over $3$ millions rows and thus takes too much time to run, I create a subsample whose size is $10\\%$ of the original training set. To make this subsample more representative, I divide the training set into $100$ categories. The $i$-th category contains comments whose the percentile of their `ups` belong to the interval $[i, i+1]$. For each category, we randomly select $10\\%$ of the total comments in that category.\n",
    "\n",
    "We use function `pandas.qcut` to get the percentile (w.r.t `ups`) of each comment. It's mentioned in the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.qcut.html):\n",
    "\n",
    ">q: Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.\n",
    "\n",
    "So to get percentile, we should set `q` equal to $100$. However, setting `q = 100` does not do what we expect. We still don't know why. With trial and error, we found that `q = 1710` produces $100$ percentile as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = pd.qcut(y_train, q = 1710, labels = False, duplicates = 'drop')\n",
    "ind_sub = []\n",
    "prop = 0.1\n",
    "for i in np.unique(percentile):\n",
    "    ind_tmp = percentile == i\n",
    "    ind_sub = ind_sub + pd.Series(y_train[ind_tmp].index).sample(frac = prop).tolist()\n",
    "sub_X_train, sub_y_train = X_train.iloc[ind_sub], y_train.iloc[ind_sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_jobs = -1)\n",
    "model.fit(sub_X_train, sub_y_train)\n",
    "\n",
    "# Get the mean absolute error on the validation data\n",
    "y_pred = model.predict(X_test)\n",
    "MAE = mean_absolute_error(y_pred , y_test_full)\n",
    "print('Random forest validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBModel = XGBRegressor()\n",
    "XGBModel.fit(sub_X_train, sub_y_train, verbose = False)\n",
    "\n",
    "# Get the mean absolute error on the validation data :\n",
    "y_pred2 = XGBModel.predict(X_test)\n",
    "MAE = mean_absolute_error(y_pred2, y_test_full)\n",
    "print('XGBoost validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "model = LGBMRegressor()\n",
    "model.fit(sub_X_train, sub_y_train)\n",
    "# Get the mean absolute error on the validation data\n",
    "y_pred3 = model.predict(X_test)\n",
    "MAE = mean_absolute_error(y_pred3, y_test_full)\n",
    "print('Random forest validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r'E:\\\\M2 EconStat\\\\Web Mining\\\\prediction.csv'\n",
    "# prediction = pd.DataFrame({'id': df.loc[ind_test, 'id'], 'predicted': y_pred3})\n",
    "# prediction.to_csv(path, index = False, encoding =  'utf-8', header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
