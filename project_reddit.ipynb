{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-Cell { width: 80% !important; margin: 0 auto;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, unidecode, re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "display(HTML(\"<style>.jp-Cell { width: 80% !important; margin: 0 auto;}</style>\"))\n",
    "path = 'E:\\\\M2 EconStat\\\\Web Mining\\\\Project\\\\comments_students.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, header = 0, nrows = 100000)\n",
    "df2 = df.sort_values('ups', ascending = False, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_lowercase(df, colname):\n",
    "    df[colname] = df[colname].str.lower()\n",
    "    return df\n",
    "    \n",
    "def not_regex(pattern):\n",
    "        return r\"((?!{}).)\".format(pattern)\n",
    "    \n",
    "# Delete punctuation\n",
    "def remove_punctuation(df, colname):\n",
    "    df[colname] = df[colname].str.replace('\\n', ' ')\n",
    "    df[colname] = df[colname].str.replace('\\r', ' ')\n",
    "    df[colname] = df[colname].apply(lambda x: unidecode.unidecode(x))\n",
    "    alphanumeric_characters_extended = '(\\\\b[-/]\\\\b|[a-zA-Z0-9])'\n",
    "    df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), ' ', regex = True)\n",
    "    return df\n",
    "\n",
    "def tokenize_sentence(df, colname):\n",
    "    df[colname] = df[colname].str.split()\n",
    "    return df\n",
    "\n",
    "# Delete stop words\n",
    "def remove_stop_words(df, colname):\n",
    "    my_stopwords = stopwords.words('english')\n",
    "    df[colname] = df[colname].apply(lambda x: [word for word in x if word not in my_stopwords])\n",
    "    return df\n",
    "\n",
    "def reverse_tokenize_sentence(df, colname):\n",
    "    df[colname] = df[colname].map(lambda word: ' '.join(word))\n",
    "    return df\n",
    "\n",
    "\n",
    "def text_cleaning(df, colname):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. convert text to lowercase\n",
    "    2. remove punctuation and new line characters '\\n'\n",
    "    3. Tokenize sentences\n",
    "    4. Remove all stopwords\n",
    "    5. convert tokenized text to text\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        df\n",
    "        .pipe(convert_text_to_lowercase, colname)\n",
    "        .pipe(remove_punctuation, colname)\n",
    "        .pipe(tokenize_sentence, colname)\n",
    "        .pipe(remove_stop_words, colname)\n",
    "        .pipe(reverse_tokenize_sentence, colname)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = text_cleaning(df, 'body')\n",
    "data = df3['body']\n",
    "y = df3['ups']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = 50\n",
    "\n",
    "tfi_df_vec = TfidfVectorizer(max_df = 0.5,\n",
    "                             min_df = 2,\n",
    "                             use_idf = True,\n",
    "                             stop_words = 'english',\n",
    "                             max_features = n_feature)\n",
    "\n",
    "X = tfi_df_vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.33,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 8008      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,009,017\n",
      "Trainable params: 1,009,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim = 1000, activation = 'relu', kernel_initializer='normal'))\n",
    "model.add(Dense(8, activation = 'relu', kernel_initializer='normal'))\n",
    "model.add(Dense(1, activation = 'linear', kernel_initializer='normal'))\n",
    "model.compile(loss = 'mean_absolute_error',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "# model.fit(X_train, y_train,\n",
    "#           epochs = 3,\n",
    "#           batch_size = 10,\n",
    "#           validation_data = (X_test, y_test),\n",
    "#           verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest validation MAE =  21.653275848425533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the mean absolute error on the validation data\n",
    "predicted_prices = model.predict(X_test)\n",
    "MAE = mean_absolute_error(y_test , predicted_prices)\n",
    "print('Random forest validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost validation MAE =  22.30182093093011\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "XGBModel = XGBRegressor()\n",
    "XGBModel.fit(X_train, y_train , verbose = False)\n",
    "\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(X_test)\n",
    "MAE = mean_absolute_error(y_test , XGBpredictions)\n",
    "print('XGBoost validation MAE = ', MAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
