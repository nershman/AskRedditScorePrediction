{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-Cell { width: 80% !important; margin: 0 auto;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, unidecode, re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "display(HTML(\"<style>.jp-Cell { width: 80% !important; margin: 0 auto;}</style>\"))\n",
    "path = 'E:\\\\M2 EconStat\\\\Web Mining\\\\Project\\\\comments_students.csv'\n",
    "my_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(path, header = 0, nrows = 200000)\n",
    "#df2 = df.sort_values('ups', ascending = False, ignore_index = True)\n",
    "#np.sum(df['body'] == None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, header = 0, nrows = 10000)\n",
    "ind = (df['body'] != 'deleted') & (df['body'] != '[deleted]') & df['ups'].notna() & df['body'].notna()\n",
    "df = df[ind].astype({'body': 'string'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract more features\n",
    "\n",
    "- *Timing*: time since root, time since parent (in hours), number of later comments, and number of previous comments\n",
    "\n",
    "- *Author*: a binary indicator as to whether the author is the original poster, and number of comments made by the author in the conversation\n",
    "\n",
    "- *Graph-location*: depth of the comment (distance from the root), and number of siblings\n",
    "\n",
    "- *Graph-response*: number of children (direct replies to the comment), height of the subtree rooted from the node, size of that subtree, number of children normalized for each thread (2 normalization techniques), subtree size normalized for each thread (2 normalization techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(df, colname):\n",
    "    # Convert text to lowercase\n",
    "    tmp = df[colname].str.lower()\n",
    "    \n",
    "    # Delete punctuation\n",
    "    tmp = tmp.str.replace('\\n', ' ')\n",
    "    tmp = tmp.str.replace('\\r', ' ')    \n",
    "    tmp = tmp.str.replace(r\"((?!{}).)\".format('(\\\\b[-/]\\\\b|[a-zA-Z0-9])'), ' ', regex = True)\n",
    "    \n",
    "    # Tokenize\n",
    "    tmp = tmp.str.split()\n",
    "    \n",
    "    # Delete stop words\n",
    "    tmp = tmp.apply(lambda x: [w for w in x if w not in my_stopwords])\n",
    "    \n",
    "    # Reverse tokenize\n",
    "    df[colname] = tmp.map(lambda word: ' '.join(word))  \n",
    "    \n",
    "    return df\n",
    "\n",
    "df = text_cleaning(df, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['body']\n",
    "y = df['ups']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = 50\n",
    "\n",
    "tfi_df_vec = TfidfVectorizer(use_idf = True,\n",
    "                             max_features = n_feature)\n",
    "\n",
    "X = tfi_df_vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.33,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest validation MAE =  21.796178664497134\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the mean absolute error on the validation data\n",
    "y_pred = model.predict(X_test)\n",
    "MAE = mean_absolute_error(y_test , y_pred)\n",
    "print('Random forest validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# XGBModel = XGBRegressor()\n",
    "# XGBModel.fit(X_train, y_train , verbose = False)\n",
    "\n",
    "# # Get the mean absolute error on the validation data :\n",
    "# XGBpredictions = XGBModel.predict(X_test)\n",
    "# MAE = mean_absolute_error(y_test , XGBpredictions)\n",
    "# print('XGBoost validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(1000, input_dim = 1000, activation = 'relu', kernel_initializer='normal'))\n",
    "# model.add(Dense(8, activation = 'relu', kernel_initializer='normal'))\n",
    "# model.add(Dense(1, activation = 'linear', kernel_initializer='normal'))\n",
    "# model.compile(loss = 'mean_absolute_error',\n",
    "#               optimizer = 'adam',\n",
    "#               metrics = ['accuracy'])\n",
    "# print(model.summary())\n",
    "# model.fit(X_train, y_train,\n",
    "#           epochs = 3,\n",
    "#           batch_size = 10,\n",
    "#           validation_data = (X_test, y_test),\n",
    "#           verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
