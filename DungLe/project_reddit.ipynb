{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-Cell { width: 80% !important; margin: 0 auto;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, unidecode, re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import networkx as nx\n",
    "from multiprocessing import dummy\n",
    "from networkx.algorithms.traversal.depth_first_search import dfs_tree\n",
    "\n",
    "display(HTML(\"<style>.jp-Cell { width: 80% !important; margin: 0 auto;}</style>\"))\n",
    "path = 'E:\\\\M2 EconStat\\\\Web Mining\\\\Project\\\\comments_students.csv'\n",
    "my_stopwords = stopwords.words('english')\n",
    "P = dummy.Pool(processes = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(path, header = 0, nrows = 200000)\n",
    "#df2 = df.sort_values('ups', ascending = False, ignore_index = True)\n",
    "#df2 = df.sort_values(by=['link_id'])\n",
    "#np.sum(df['body'] == None)\n",
    "#df[ind] = df[ind].astype({'body': 'string'})\n",
    "#df['body']\n",
    "#df['body'][ind].str.lower().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, header = 0)\n",
    "\n",
    "# ind is the indices of \"normal\" rows that will be fed into the model later\n",
    "ind = (df['body'] != 'deleted') & (df['body'] != '[deleted]') & df['body'].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(df, colname, ind):\n",
    "    \n",
    "    tmp = df[colname][ind].copy()\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    tmp = tmp.str.lower()\n",
    "    \n",
    "    # Delete punctuation\n",
    "    tmp = tmp.str.replace('\\n', ' ')\n",
    "    tmp = tmp.str.replace('\\r', ' ')    \n",
    "    tmp = tmp.str.replace(r\"((?!{}).)\".format('(\\\\b[-/]\\\\b|[a-zA-Z0-9])'), ' ', regex = True)\n",
    "    \n",
    "    # Tokenize\n",
    "    tmp = tmp.str.split()\n",
    "    \n",
    "    # Delete stop words\n",
    "    tmp = tmp.apply(lambda x: [w for w in x if w not in my_stopwords])\n",
    "    \n",
    "    # Reverse tokenize\n",
    "    df.loc[ind, colname] = tmp.map(lambda word: ' '.join(word))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = text_cleaning(df, 'body', ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate subgraphs and extract more features for each comment\n",
    "\n",
    "- *Timing*: time since root, time since parent (in hours), number of later comments, and number of previous comments\n",
    "\n",
    "- *Author*: a binary indicator as to whether the author is the original poster, and number of comments made by the author in the conversation\n",
    "\n",
    "- *Graph-location*: depth of the comment (distance from the root), and number of siblings\n",
    "\n",
    "- *Graph-response*: number of children (direct replies to the comment), height of the subtree rooted from the node, size of that subtree, number of children normalized for each thread (2 normalization techniques), subtree size normalized for each thread (2 normalization techniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_utc': 1431317199,\n",
       " 'ups': 6761.0,\n",
       " 'link_id': 't3_35jfjt',\n",
       " 'author': 'buckus69',\n",
       " 'body': 'got one night standoff',\n",
       " 'depth': 3,\n",
       " 'num_siblings': 67,\n",
       " 'num_children': 214,\n",
       " 'comments_in_subtree': 311,\n",
       " 'height_subtree': 7,\n",
       " 'num_comments_by_author': 8,\n",
       " 'num_previous_comments': 1393,\n",
       " 'num_later_comments': 11324}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_ids = np.unique(df['link_id']).tolist()  \n",
    "atts = ['created_utc', 'ups', 'link_id', 'author', 'body', 'parent_id']\n",
    "\n",
    "def create_subgraph(link_id):\n",
    "    ind = df['link_id'] == link_id\n",
    "    temp_df = df[ind]\n",
    "    g = nx.from_pandas_edgelist(temp_df, source = 'parent_id', target = 'name', create_using = nx.DiGraph())\n",
    "    \n",
    "    # Features that already exist in original dataframe\n",
    "    for att in atts:\n",
    "        nx.set_node_attributes(g,\n",
    "                               temp_df[['name', att]].set_index('name').T.to_dict('records')[0],\n",
    "                               name = att)\n",
    "    \n",
    "    ## Graph-location\n",
    "    # Depth of a comment (distance from the root)    \n",
    "    nx.set_node_attributes(g, nx.shortest_path_length(g, link_id), name = 'depth')\n",
    "    \n",
    "    # Number of siblings\n",
    "    nx.set_node_attributes(g,\n",
    "                           dict(zip(temp_df.name, temp_df.groupby('parent_id')['parent_id'].transform('count') - 1)),\n",
    "                           name = 'num_siblings')\n",
    "    \n",
    "    ## Graph-response\n",
    "    # Number of children (direct replies to a comment)\n",
    "    nx.set_node_attributes(g,\n",
    "                           temp_df.groupby(by = 'parent_id').size().reindex(temp_df['name'], fill_value = 0).to_dict(),\n",
    "                           name = 'num_children')\n",
    "    \n",
    "    num_comment = dict.fromkeys(g.nodes, 0)\n",
    "    height_subtree = dict.fromkeys(g.nodes, 0)\n",
    "    \n",
    "    for node in g.nodes:\n",
    "        sub_tree = dfs_tree(g, node)\n",
    "        num_comment[node] = sub_tree.number_of_nodes()\n",
    "        height_subtree[node] = nx.dag_longest_path_length(sub_tree)\n",
    "        \n",
    "    # Number of comments in the subtree rooted from the comment\n",
    "    nx.set_node_attributes(g, num_comment, name = 'comments_in_subtree')\n",
    "        \n",
    "\n",
    "    # Height of the subtree rooted from the comment\n",
    "    nx.set_node_attributes(g, height_subtree, name = 'height_subtree')    \n",
    "    \n",
    "    ## Author\n",
    "    # Number of comments made by the author in the conversation\n",
    "    nx.set_node_attributes(g,\n",
    "                           dict(zip(temp_df.name, temp_df.groupby('author')['author'].transform('count'))),\n",
    "                           name = 'num_comments_by_author')\n",
    "    \n",
    "    ## Timing\n",
    "    count_utc = temp_df.groupby('created_utc').size()\n",
    "    \n",
    "    # Number of previous comments\n",
    "    cum_counts = count_utc.sort_index(ascending = True).shift(fill_value = 0).cumsum()\n",
    "    nx.set_node_attributes(g,\n",
    "                           dict(zip(temp_df['name'], temp_df['created_utc'].map(cum_counts))),\n",
    "                           name = 'num_previous_comments')  \n",
    "    \n",
    "    # number of later comments\n",
    "    cum_counts = count_utc.sort_index(ascending = False).shift(fill_value = 0).cumsum()\n",
    "    nx.set_node_attributes(g,\n",
    "                           dict(zip(temp_df['name'], temp_df['created_utc'].map(cum_counts))),\n",
    "                           name = 'num_later_comments')\n",
    "    \n",
    "    # Time since root (in unix timestamp https://www.epochconverter.com/)\n",
    "    #intuitively, the earliest comment in our subgraph will be the root. \n",
    "    #we take the minimum of the times and subtract that from the time of comment to calculate the time since root.\n",
    "    roottime = np.min(temp_df.created_utc)\n",
    "    mytime = temp_df.\n",
    "    nx.set_node_attributes(g,\n",
    "                           dict(zip(temp_df['id'], abs(temp_df['created_utc'] - roottime))),\n",
    "                           name = 'time_since_root')\n",
    "    \n",
    "    # Time since parent (in hours)\n",
    "    \n",
    "    nx.set_node_attributes(g, #not sure if this will work, the iloc.\n",
    "                           dict(zip(temp_df['id'], abs(temp_df['created_utc'] - temp_df.iloc['parent_id']['created_utc']))),\n",
    "                           name = 'time_since_parent')\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = create_subgraph('t3_35jfjt')\n",
    "g.nodes['t1_cr56nez']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['body']\n",
    "y = df['ups']\n",
    "\n",
    "n_feature = 50\n",
    "\n",
    "tfi_df_vec = TfidfVectorizer(use_idf = True,\n",
    "                             max_features = n_feature)\n",
    "\n",
    "X = tfi_df_vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.33,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest validation MAE =  21.796178664497134\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the mean absolute error on the validation data\n",
    "y_pred = model.predict(X_test)\n",
    "MAE = mean_absolute_error(y_test , y_pred)\n",
    "print('Random forest validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# XGBModel = XGBRegressor()\n",
    "# XGBModel.fit(X_train, y_train , verbose = False)\n",
    "\n",
    "# # Get the mean absolute error on the validation data :\n",
    "# XGBpredictions = XGBModel.predict(X_test)\n",
    "# MAE = mean_absolute_error(y_test , XGBpredictions)\n",
    "# print('XGBoost validation MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(1000, input_dim = 1000, activation = 'relu', kernel_initializer='normal'))\n",
    "# model.add(Dense(8, activation = 'relu', kernel_initializer='normal'))\n",
    "# model.add(Dense(1, activation = 'linear', kernel_initializer='normal'))\n",
    "# model.compile(loss = 'mean_absolute_error',\n",
    "#               optimizer = 'adam',\n",
    "#               metrics = ['accuracy'])\n",
    "# print(model.summary())\n",
    "# model.fit(X_train, y_train,\n",
    "#           epochs = 3,\n",
    "#           batch_size = 10,\n",
    "#           validation_data = (X_test, y_test),\n",
    "#           verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
